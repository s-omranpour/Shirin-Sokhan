{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a6477d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    ")\n",
    "\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214c3e0",
   "metadata": {},
   "source": [
    "## tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10893bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"HooshvareLab/gpt2-fa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    \"bos_token\": '</s>',\n",
    "    \"eos_token\": '</s>', \n",
    "    \"pad_token\": '<pad>',\n",
    "    \"unk_token\": '<unk>',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56ef3e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11579631"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.data\n",
    "reload(src.data)\n",
    "from src.data import PoemDataset, get_dataloaders\n",
    "\n",
    "dataset = PoemDataset(tokenizer, 'data/all_poems.json', max_len=512)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd28cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 10421668 samples and val dataset has 1157963 samples\n"
     ]
    }
   ],
   "source": [
    "tl, vl = get_dataloaders(dataset, val_frac=0.1, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00667bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([32, 186])\n",
      "attention_mask torch.Size([32, 186])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(tl))\n",
    "for k in b:\n",
    "    print(k, b[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46511bb",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "387b45ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained model: HooshvareLab/gpt2-fa\n"
     ]
    }
   ],
   "source": [
    "import src.model\n",
    "reload(src.model)\n",
    "from src.model import PoetFormer\n",
    "\n",
    "model = PoetFormer(pretrained=\"HooshvareLab/gpt2-fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48865c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118099200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e92dcabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حافظ:\n",
      "جغند. از این بابت به من خیلی حق می‌دهند. من هم از این قاعده مستثنی نیستم. اما اگر کسی بود که می‌گفت تو آدم خیلی خفنی هستی که نمی‌تونی از پسش بربیای. چرا باید برای تو می‌جنگیم؟ آیا واقعا باید برای من جنگ باشه؟ چرا باید به من جنگی یاد بده؟ اگر به تو جنگ‌های زیادی یاد بدهم، آیا می‌توانم مثل آنها زندگی کنم؟ این دو سوال را در این متن با هم مرور می‌کنیم. خیلی از افراد که می‌خواهند در مورد این موضوع صحبت کنند، می‌پرسند که چرا من یک\n"
     ]
    }
   ],
   "source": [
    "res = model.generate(num_return_sequences=1, max_length=128, n_beam=1)\n",
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab700f",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "127a5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: GPT2-fa-ganjoor-conditional\n"
     ]
    }
   ],
   "source": [
    "name = 'GPT2-fa-ganjoor-conditional'\n",
    "print('model name:',name)\n",
    "\n",
    "logger = TensorBoardLogger(save_dir='logs/', name=name)\n",
    "lr_logger = LearningRateMonitor(logging_interval='step')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=f'weights/{name}/', \n",
    "    filename='{epoch}-{val_loss:.2f}', \n",
    "    monitor='val_loss',\n",
    "    save_top_k=1, \n",
    "    period=1\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    benchmark=True, \n",
    "    gpus=0, \n",
    "    accumulate_grad_batches=1,\n",
    "    logger=logger, \n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint, lr_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3864cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 118 M \n",
      "------------------------------------------\n",
      "118 M     Trainable params\n",
      "0         Non-trainable params\n",
      "118 M     Total params\n",
      "472.397   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19370db6cc504442b6d4433efa95fa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-23:\n",
      "Process Process-21:\n",
      "Process Process-24:\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f54276311f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/soroosh/projects/general_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/soroosh/projects/general_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Process Process-22:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, tl, vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65101f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
